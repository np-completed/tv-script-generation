
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dlnd\_tv\_script\_generation}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{TV Script Generation}\label{tv-script-generation}

In this project, you'll generate your own
\href{https://en.wikipedia.org/wiki/The_Simpsons}{Simpsons} TV scripts
using RNNs. You'll be using part of the
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{Simpsons
dataset} of scripts from 27 seasons. The Neural Network you'll build
will generate a new TV script for a scene at
\href{https://simpsonswiki.com/wiki/Moe's_Tavern}{Moe's Tavern}. \#\#
Get the Data The data is already provided for you. You'll be using a
subset of the original dataset. It consists of only the scenes in Moe's
Tavern. This doesn't include other versions of the tavern, like "Moe's
Cavern", "Flaming Moe's", "Uncle Moe's Family Feed-Bag", etc..

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        
        \PY{n}{data\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/simpsons/moes\PYZus{}tavern\PYZus{}lines.txt}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{text} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Ignore notice, since we don\PYZsq{}t use it for analysing the data}
        \PY{n}{text} \PY{o}{=} \PY{n}{text}\PY{p}{[}\PY{l+m+mi}{81}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \subsection{Explore the Data}\label{explore-the-data}

Play around with \texttt{view\_sentence\_range} to view different parts
of the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{view\PYZus{}sentence\PYZus{}range} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dataset Stats}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Roughly the number of unique words: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{p}{\PYZob{}}\PY{n}{word}\PY{p}{:} \PY{k+kc}{None} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{scenes} \PY{o}{=} \PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of scenes: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scenes}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{sentence\PYZus{}count\PYZus{}scene} \PY{o}{=} \PY{p}{[}\PY{n}{scene}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of sentences in each scene: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{sentence\PYZus{}count\PYZus{}scene}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{sentences} \PY{o}{=} \PY{p}{[}\PY{n}{sentence} \PY{k}{for} \PY{n}{scene} \PY{o+ow}{in} \PY{n}{scenes} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{scene}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of lines: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentences}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{word\PYZus{}count\PYZus{}sentence} \PY{o}{=} \PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{sentences}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average number of words in each line: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{word\PYZus{}count\PYZus{}sentence}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The sentences }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ to }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{o}{*}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{text}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}\PY{n}{view\PYZus{}sentence\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsection{Implement Preprocessing
Functions}\label{implement-preprocessing-functions}

The first thing to do to any dataset is preprocessing. Implement the
following preprocessing functions below: - Lookup Table - Tokenize
Punctuation

\subsubsection{Lookup Table}\label{lookup-table}

To create a word embedding, you first need to transform the words to
ids. In this function, create two dictionaries: - Dictionary to go from
the words to an id, we'll call \texttt{vocab\_to\_int} - Dictionary to
go from the id to word, we'll call \texttt{int\_to\_vocab}

Return these dictionaries in the following tuple
\texttt{(vocab\_to\_int,\ int\_to\_vocab)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        
        \PY{k}{def} \PY{n+nf}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create lookup tables for vocabulary}
        \PY{l+s+sd}{    :param text: The text of tv scripts split into words}
        \PY{l+s+sd}{    :return: A tuple of dicts (vocab\PYZus{}to\PYZus{}int, int\PYZus{}to\PYZus{}vocab)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}create\PYZus{}lookup\PYZus{}tables}\PY{p}{(}\PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Tokenize Punctuation}\label{tokenize-punctuation}

We'll be splitting the script into a word array using spaces as
delimiters. However, punctuations like periods and exclamation marks
make it hard for the neural network to distinguish between the word
"bye" and "bye!".

Implement the function \texttt{token\_lookup} to return a dict that will
be used to tokenize symbols like "!" into
"\textbar{}\textbar{}Exclamation\_Mark\textbar{}\textbar{}". Create a
dictionary for the following symbols where the symbol is the key and
value is the token: - Period ( . ) - Comma ( , ) - Quotation Mark ( " )
- Semicolon ( ; ) - Exclamation mark ( ! ) - Question mark ( ? ) - Left
Parentheses ( ( ) - Right Parentheses ( ) ) - Dash ( -\/- ) - Return (
\n )

This dictionary will be used to token the symbols and add the delimiter
(space) around it. This separates the symbols as it's own word, making
it easier for the neural network to predict on the next word. Make sure
you don't use a token that could be confused as a word. Instead of using
the token "dash", try using something like
"\textbar{}\textbar{}dash\textbar{}\textbar{}".

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{token\PYZus{}lookup}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Generate a dict to turn punctuation into a token.}
        \PY{l+s+sd}{    :return: Tokenize dictionary where the key is the punctuation and the value is the token}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}tokenize}\PY{p}{(}\PY{n}{token\PYZus{}lookup}\PY{p}{)}
\end{Verbatim}


    \subsection{Preprocess all the data and save
it}\label{preprocess-all-the-data-and-save-it}

Running the code cell below will preprocess all the data and save it to
file.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Preprocess Training, Validation, and Testing Data}
        \PY{n}{helper}\PY{o}{.}\PY{n}{preprocess\PYZus{}and\PYZus{}save\PYZus{}data}\PY{p}{(}\PY{n}{data\PYZus{}dir}\PY{p}{,} \PY{n}{token\PYZus{}lookup}\PY{p}{,} \PY{n}{create\PYZus{}lookup\PYZus{}tables}\PY{p}{)}
\end{Verbatim}


    \section{Check Point}\label{check-point}

This is your first checkpoint. If you ever decide to come back to this
notebook or have to restart the notebook, you can start from here. The
preprocessed data has been saved to disk.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        
        \PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \subsection{Build the Neural Network}\label{build-the-neural-network}

You'll build the components necessary to build a RNN by implementing the
following functions below: - get\_inputs - get\_init\_cell - get\_embed
- build\_rnn - build\_nn - get\_batches

\subsubsection{Check the Version of TensorFlow and Access to
GPU}\label{check-the-version-of-tensorflow-and-access-to-gpu}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{from} \PY{n+nn}{distutils}\PY{n+nn}{.}\PY{n+nn}{version} \PY{k}{import} \PY{n}{LooseVersion}
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        
        \PY{c+c1}{\PYZsh{} Check TensorFlow Version}
        \PY{k}{assert} \PY{n}{LooseVersion}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{LooseVersion}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1.0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Please use TensorFlow version 1.0 or newer}\PY{l+s+s1}{\PYZsq{}}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow Version: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Check for a GPU}
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{warnings}\PY{o}{.}\PY{n}{warn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No GPU found. Please use a GPU to train your neural network.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Default GPU Device: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{test}\PY{o}{.}\PY{n}{gpu\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Input}\label{input}

Implement the \texttt{get\_inputs()} function to create TF Placeholders
for the Neural Network. It should create the following placeholders: -
Input text placeholder named "input" using the
\href{https://www.tensorflow.org/api_docs/python/tf/placeholder}{TF
Placeholder} \texttt{name} parameter. - Targets placeholder - Learning
Rate placeholder

Return the placeholders in the following tuple
\texttt{(Input,\ Targets,\ LearningRate)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create TF Placeholders for input, targets, and learning rate.}
        \PY{l+s+sd}{    :return: Tuple (input, targets, learning rate)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}inputs}\PY{p}{(}\PY{n}{get\PYZus{}inputs}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Build RNN Cell and
Initialize}\label{build-rnn-cell-and-initialize}

Stack one or more
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell}{\texttt{BasicLSTMCells}}
in a
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell}{\texttt{MultiRNNCell}}.
- The Rnn size should be set using \texttt{rnn\_size} - Initalize Cell
State using the MultiRNNCell's
\href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell\#zero_state}{\texttt{zero\_state()}}
function - Apply the name "initial\_state" to the initial state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the cell and initial state in the following tuple
\texttt{(Cell,\ InitialState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create an RNN Cell and initialize it.}
        \PY{l+s+sd}{    :param batch\PYZus{}size: Size of batches}
        \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of RNNs}
        \PY{l+s+sd}{    :return: Tuple (cell, initialize state)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Word Embedding}\label{word-embedding}

Apply embedding to \texttt{input\_data} using TensorFlow. Return the
embedded sequence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}embed}\PY{p}{(}\PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create embedding for \PYZlt{}input\PYZus{}data\PYZgt{}.}
        \PY{l+s+sd}{    :param input\PYZus{}data: TF placeholder for text input.}
        \PY{l+s+sd}{    :param vocab\PYZus{}size: Number of words in vocabulary.}
        \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
        \PY{l+s+sd}{    :return: Embedded input.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}embed}\PY{p}{(}\PY{n}{get\PYZus{}embed}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Build RNN}\label{build-rnn}

You created a RNN Cell in the \texttt{get\_init\_cell()} function. Time
to use the cell to create a RNN. - Build the RNN using the
\href{https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn}{\texttt{tf.nn.dynamic\_rnn()}}
- Apply the name "final\_state" to the final state using
\href{https://www.tensorflow.org/api_docs/python/tf/identity}{\texttt{tf.identity()}}

Return the outputs and final\_state state in the following tuple
\texttt{(Outputs,\ FinalState)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{build\PYZus{}rnn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{inputs}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Create a RNN using a RNN Cell}
        \PY{l+s+sd}{    :param cell: RNN Cell}
        \PY{l+s+sd}{    :param inputs: Input text data}
        \PY{l+s+sd}{    :return: Tuple (Outputs, Final State)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}rnn}\PY{p}{(}\PY{n}{build\PYZus{}rnn}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Build the Neural Network}\label{build-the-neural-network}

Apply the functions you implemented above to: - Apply embedding to
\texttt{input\_data} using your
\texttt{get\_embed(input\_data,\ vocab\_size,\ embed\_dim)} function. -
Build RNN using \texttt{cell} and your
\texttt{build\_rnn(cell,\ inputs)} function. - Apply a fully connected
layer with a linear activation and \texttt{vocab\_size} as the number of
outputs.

Return the logits and final state in the following tuple (Logits,
FinalState)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}data}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Build part of the neural network}
        \PY{l+s+sd}{    :param cell: RNN cell}
        \PY{l+s+sd}{    :param rnn\PYZus{}size: Size of rnns}
        \PY{l+s+sd}{    :param input\PYZus{}data: Input data}
        \PY{l+s+sd}{    :param vocab\PYZus{}size: Vocabulary size}
        \PY{l+s+sd}{    :param embed\PYZus{}dim: Number of embedding dimensions}
        \PY{l+s+sd}{    :return: Tuple (Logits, FinalState)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}build\PYZus{}nn}\PY{p}{(}\PY{n}{build\PYZus{}nn}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Batches}\label{batches}

Implement \texttt{get\_batches} to create batches of input and targets
using \texttt{int\_text}. The batches should be a Numpy array with the
shape
\texttt{(number\ of\ batches,\ 2,\ batch\ size,\ sequence\ length)}.
Each batch contains two elements: - The first element is a single batch
of \textbf{input} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}} - The second element is a
single batch of \textbf{targets} with the shape
\texttt{{[}batch\ size,\ sequence\ length{]}}

If you can't fill the last batch with enough data, drop the last batch.

For example,
\texttt{get\_batches({[}1,\ 2,\ 3,\ 4,\ 5,\ 6,\ 7,\ 8,\ 9,\ 10,\ 11,\ 12,\ 13,\ 14,\ 15,\ 16,\ 17,\ 18,\ 19,\ 20{]},\ 3,\ 2)}
would return a Numpy array of the following:

\begin{verbatim}
[
  # First Batch
  [
    # Batch of Input
    [[ 1  2], [ 7  8], [13 14]]
    # Batch of targets
    [[ 2  3], [ 8  9], [14 15]]
  ]

  # Second Batch
  [
    # Batch of Input
    [[ 3  4], [ 9 10], [15 16]]
    # Batch of targets
    [[ 4  5], [10 11], [16 17]]
  ]

  # Third Batch
  [
    # Batch of Input
    [[ 5  6], [11 12], [17 18]]
    # Batch of targets
    [[ 6  7], [12 13], [18  1]]
  ]
]
\end{verbatim}

Notice that the last target value in the last batch is the first input
value of the first batch. In this case, \texttt{1}. This is a common
technique used when creating sequence batches, although it is rather
unintuitive.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Return batches of input and target}
        \PY{l+s+sd}{    :param int\PYZus{}text: Text with the words replaced by their ids}
        \PY{l+s+sd}{    :param batch\PYZus{}size: The size of batch}
        \PY{l+s+sd}{    :param seq\PYZus{}length: The length of sequence}
        \PY{l+s+sd}{    :return: Batches as a Numpy array}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}batches}\PY{p}{(}\PY{n}{get\PYZus{}batches}\PY{p}{)}
\end{Verbatim}


    \subsection{Neural Network Training}\label{neural-network-training}

\subsubsection{Hyperparameters}\label{hyperparameters}

Tune the following parameters:

\begin{itemize}
\tightlist
\item
  Set \texttt{num\_epochs} to the number of epochs.
\item
  Set \texttt{batch\_size} to the batch size.
\item
  Set \texttt{rnn\_size} to the size of the RNNs.
\item
  Set \texttt{embed\_dim} to the size of the embedding.
\item
  Set \texttt{seq\_length} to the length of sequence.
\item
  Set \texttt{learning\_rate} to the learning rate.
\item
  Set \texttt{show\_every\_n\_batches} to the number of batches the
  neural network should print progress.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Number of Epochs}
        \PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} Batch Size}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} RNN Size}
        \PY{n}{rnn\PYZus{}size} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} Embedding Dimension Size}
        \PY{n}{embed\PYZus{}dim} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} Sequence Length}
        \PY{n}{seq\PYZus{}length} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} Learning Rate}
        \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{} Show stats for every n number of batches}
        \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{=} \PY{k+kc}{None}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{save\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./save}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \subsubsection{Build the Graph}\label{build-the-graph}

Build the graph using the neural network you implemented.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{contrib} \PY{k}{import} \PY{n}{seq2seq}
        
        \PY{n}{train\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
        \PY{k}{with} \PY{n}{train\PYZus{}graph}\PY{o}{.}\PY{n}{as\PYZus{}default}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{vocab\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
            \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{targets}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{n}{get\PYZus{}inputs}\PY{p}{(}\PY{p}{)}
            \PY{n}{input\PYZus{}data\PYZus{}shape} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{input\PYZus{}text}\PY{p}{)}
            \PY{n}{cell}\PY{p}{,} \PY{n}{initial\PYZus{}state} \PY{o}{=} \PY{n}{get\PYZus{}init\PYZus{}cell}\PY{p}{(}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{)}
            \PY{n}{logits}\PY{p}{,} \PY{n}{final\PYZus{}state} \PY{o}{=} \PY{n}{build\PYZus{}nn}\PY{p}{(}\PY{n}{cell}\PY{p}{,} \PY{n}{rnn\PYZus{}size}\PY{p}{,} \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embed\PYZus{}dim}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Probabilities for generating words}
            \PY{n}{probs} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{logits}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{probs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Loss function}
            \PY{n}{cost} \PY{o}{=} \PY{n}{seq2seq}\PY{o}{.}\PY{n}{sequence\PYZus{}loss}\PY{p}{(}
                \PY{n}{logits}\PY{p}{,}
                \PY{n}{targets}\PY{p}{,}
                \PY{n}{tf}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{input\PYZus{}data\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Optimizer}
            \PY{n}{optimizer} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{AdamOptimizer}\PY{p}{(}\PY{n}{lr}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Gradient Clipping}
            \PY{n}{gradients} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{compute\PYZus{}gradients}\PY{p}{(}\PY{n}{cost}\PY{p}{)}
            \PY{n}{capped\PYZus{}gradients} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{clip\PYZus{}by\PYZus{}value}\PY{p}{(}\PY{n}{grad}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{l+m+mf}{1.}\PY{p}{)}\PY{p}{,} \PY{n}{var}\PY{p}{)} \PY{k}{for} \PY{n}{grad}\PY{p}{,} \PY{n}{var} \PY{o+ow}{in} \PY{n}{gradients} \PY{k}{if} \PY{n}{grad} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}
            \PY{n}{train\PYZus{}op} \PY{o}{=} \PY{n}{optimizer}\PY{o}{.}\PY{n}{apply\PYZus{}gradients}\PY{p}{(}\PY{n}{capped\PYZus{}gradients}\PY{p}{)}
\end{Verbatim}


    \subsection{Train}\label{train}

Train the neural network on the preprocessed data. If you have a hard
time getting a good loss, check the
\href{https://discussions.udacity.com/}{forums} to see if anyone is
having the same problem.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{batches} \PY{o}{=} \PY{n}{get\PYZus{}batches}\PY{p}{(}\PY{n}{int\PYZus{}text}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seq\PYZus{}length}\PY{p}{)}
        
        \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{train\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
            \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{tf}\PY{o}{.}\PY{n}{global\PYZus{}variables\PYZus{}initializer}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{for} \PY{n}{epoch\PYZus{}i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
                \PY{n}{state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{batches}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
        
                \PY{k}{for} \PY{n}{batch\PYZus{}i}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{:}
                    \PY{n}{feed} \PY{o}{=} \PY{p}{\PYZob{}}
                        \PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{x}\PY{p}{,}
                        \PY{n}{targets}\PY{p}{:} \PY{n}{y}\PY{p}{,}
                        \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{state}\PY{p}{,}
                        \PY{n}{lr}\PY{p}{:} \PY{n}{learning\PYZus{}rate}\PY{p}{\PYZcb{}}
                    \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{p}{[}\PY{n}{cost}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{train\PYZus{}op}\PY{p}{]}\PY{p}{,} \PY{n}{feed}\PY{p}{)}
        
                    \PY{c+c1}{\PYZsh{} Show every \PYZlt{}show\PYZus{}every\PYZus{}n\PYZus{}batches\PYZgt{} batches}
                    \PY{k}{if} \PY{p}{(}\PY{n}{epoch\PYZus{}i} \PY{o}{*} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)} \PY{o}{+} \PY{n}{batch\PYZus{}i}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{show\PYZus{}every\PYZus{}n\PYZus{}batches} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch }\PY{l+s+si}{\PYZob{}:\PYZgt{}3\PYZcb{}}\PY{l+s+s1}{ Batch }\PY{l+s+si}{\PYZob{}:\PYZgt{}4\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{   train\PYZus{}loss = }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                            \PY{n}{epoch\PYZus{}i}\PY{p}{,}
                            \PY{n}{batch\PYZus{}i}\PY{p}{,}
                            \PY{n+nb}{len}\PY{p}{(}\PY{n}{batches}\PY{p}{)}\PY{p}{,}
                            \PY{n}{train\PYZus{}loss}\PY{p}{)}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Save Model}
            \PY{n}{saver} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{Saver}\PY{p}{(}\PY{p}{)}
            \PY{n}{saver}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model Trained and Saved}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsection{Save Parameters}\label{save-parameters}

Save \texttt{seq\_length} and \texttt{save\_dir} for generating a new TV
script.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Save parameters for checkpoint}
        \PY{n}{helper}\PY{o}{.}\PY{n}{save\PYZus{}params}\PY{p}{(}\PY{p}{(}\PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \section{Checkpoint}\label{checkpoint}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{helper}
        \PY{k+kn}{import} \PY{n+nn}{problem\PYZus{}unittests} \PY{k}{as} \PY{n+nn}{tests}
        
        \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{,} \PY{n}{token\PYZus{}dict} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}preprocess}\PY{p}{(}\PY{p}{)}
        \PY{n}{seq\PYZus{}length}\PY{p}{,} \PY{n}{load\PYZus{}dir} \PY{o}{=} \PY{n}{helper}\PY{o}{.}\PY{n}{load\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \subsection{Implement Generate
Functions}\label{implement-generate-functions}

\subsubsection{Get Tensors}\label{get-tensors}

Get tensors from \texttt{loaded\_graph} using the function
\href{https://www.tensorflow.org/api_docs/python/tf/Graph\#get_tensor_by_name}{\texttt{get\_tensor\_by\_name()}}.
Get the tensors using the following names: - "input:0" -
"initial\_state:0" - "final\_state:0" - "probs:0"

Return the tensors in the following tuple
\texttt{(InputTensor,\ InitialStateTensor,\ FinalStateTensor,\ ProbsTensor)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Get input, initial state, final state, and probabilities tensor from \PYZlt{}loaded\PYZus{}graph\PYZgt{}}
        \PY{l+s+sd}{    :param loaded\PYZus{}graph: TensorFlow graph loaded from file}
        \PY{l+s+sd}{    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}get\PYZus{}tensors}\PY{p}{(}\PY{n}{get\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Choose Word}\label{choose-word}

Implement the \texttt{pick\_word()} function to select the next word
using \texttt{probabilities}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Pick the next word in the generated text}
        \PY{l+s+sd}{    :param probabilities: Probabilites of the next word}
        \PY{l+s+sd}{    :param int\PYZus{}to\PYZus{}vocab: Dictionary of word ids as the keys and words as the values}
        \PY{l+s+sd}{    :return: String of the predicted word}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{c+c1}{\PYZsh{} TODO: Implement Function}
            \PY{k}{return} \PY{k+kc}{None}
        
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{tests}\PY{o}{.}\PY{n}{test\PYZus{}pick\PYZus{}word}\PY{p}{(}\PY{n}{pick\PYZus{}word}\PY{p}{)}
\end{Verbatim}


    \subsection{Generate TV Script}\label{generate-tv-script}

This will generate the TV script for you. Set \texttt{gen\_length} to
the length of TV script you want to generate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{gen\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{200}
        \PY{c+c1}{\PYZsh{} homer\PYZus{}simpson, moe\PYZus{}szyslak, or Barney\PYZus{}Gumble}
        \PY{n}{prime\PYZus{}word} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{moe\PYZus{}szyslak}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{DON\PYZsq{}T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{n}{loaded\PYZus{}graph} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Graph}\PY{p}{(}\PY{p}{)}
        \PY{k}{with} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{graph}\PY{o}{=}\PY{n}{loaded\PYZus{}graph}\PY{p}{)} \PY{k}{as} \PY{n}{sess}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Load saved model}
            \PY{n}{loader} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{train}\PY{o}{.}\PY{n}{import\PYZus{}meta\PYZus{}graph}\PY{p}{(}\PY{n}{load\PYZus{}dir} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.meta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{loader}\PY{o}{.}\PY{n}{restore}\PY{p}{(}\PY{n}{sess}\PY{p}{,} \PY{n}{load\PYZus{}dir}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Get Tensors from loaded model}
            \PY{n}{input\PYZus{}text}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{,} \PY{n}{probs} \PY{o}{=} \PY{n}{get\PYZus{}tensors}\PY{p}{(}\PY{n}{loaded\PYZus{}graph}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Sentences generation setup}
            \PY{n}{gen\PYZus{}sentences} \PY{o}{=} \PY{p}{[}\PY{n}{prime\PYZus{}word} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
            \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{initial\PYZus{}state}\PY{p}{,} \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} Generate sentences}
            \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{gen\PYZus{}length}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Dynamic Input}
                \PY{n}{dyn\PYZus{}input} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{vocab\PYZus{}to\PYZus{}int}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{gen\PYZus{}sentences}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{n}{seq\PYZus{}length}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{]}
                \PY{n}{dyn\PYZus{}seq\PYZus{}length} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dyn\PYZus{}input}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Get Prediction}
                \PY{n}{probabilities}\PY{p}{,} \PY{n}{prev\PYZus{}state} \PY{o}{=} \PY{n}{sess}\PY{o}{.}\PY{n}{run}\PY{p}{(}
                    \PY{p}{[}\PY{n}{probs}\PY{p}{,} \PY{n}{final\PYZus{}state}\PY{p}{]}\PY{p}{,}
                    \PY{p}{\PYZob{}}\PY{n}{input\PYZus{}text}\PY{p}{:} \PY{n}{dyn\PYZus{}input}\PY{p}{,} \PY{n}{initial\PYZus{}state}\PY{p}{:} \PY{n}{prev\PYZus{}state}\PY{p}{\PYZcb{}}\PY{p}{)}
                
                \PY{n}{pred\PYZus{}word} \PY{o}{=} \PY{n}{pick\PYZus{}word}\PY{p}{(}\PY{n}{probabilities}\PY{p}{[}\PY{n}{dyn\PYZus{}seq\PYZus{}length}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{int\PYZus{}to\PYZus{}vocab}\PY{p}{)}
        
                \PY{n}{gen\PYZus{}sentences}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{pred\PYZus{}word}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Remove tokens}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{gen\PYZus{}sentences}\PY{p}{)}
            \PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{token} \PY{o+ow}{in} \PY{n}{token\PYZus{}dict}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                \PY{n}{ending} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{key} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
                \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{token}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{p}{)}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{tv\PYZus{}script} \PY{o}{=} \PY{n}{tv\PYZus{}script}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{( }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{tv\PYZus{}script}\PY{p}{)}
\end{Verbatim}


    \section{The TV Script is
Nonsensical}\label{the-tv-script-is-nonsensical}

It's ok if the TV script doesn't make any sense. We trained on less than
a megabyte of text. In order to get good results, you'll have to use a
smaller vocabulary or get more data. Luckily there's more data! As we
mentioned in the beggining of this project, this is a subset of
\href{https://www.kaggle.com/wcukierski/the-simpsons-by-the-data}{another
dataset}. We didn't have you train on all the data, because that would
take too long. However, you are free to train your neural network on all
the data. After you complete the project, of course. \# Submitting This
Project When submitting this project, make sure to run all the cells
before saving the notebook. Save the notebook file as
"dlnd\_tv\_script\_generation.ipynb" and save it as a HTML file under
"File" -\textgreater{} "Download as". Include the "helper.py" and
"problem\_unittests.py" files in your submission.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
